{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnI_EsWXnJqD"
      },
      "source": [
        "# **Asignatura**: Aprendizaje Automático\n",
        "\n",
        "**Proyecto Práctico Opcional**: Aprendizaje por Refuerzo\n",
        "\n",
        "**Nombre**: Ramón"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR9vEYARnkXx"
      },
      "source": [
        "# **¿Qué es el aprendizaje por refuerzo? (2 punto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX_DNsnJd5ff"
      },
      "source": [
        "Realizad una búsqueda bibliográfica sobre qué es el aprendizaje por refuerzo en *Machine Learning*.\n",
        "\n",
        "A continuación, describid qué es el aprendizaje por refuerzo (usad vuestras propias palabras, **NO ESTÁ PERMITIDO** el uso de IA Generativa para realizar este apartado).\n",
        "\n",
        "En particular, el texto escrito debe contener información que pueda responder a las siguientes preguntas:\n",
        "\n",
        "- ¿Dónde se encuadra el aprendizaje por refuerzo dentro del aprendizaje automático?\n",
        "- ¿Qué dos elementos principales hay involucrados en el aprendizaje por refuerzo?\n",
        "- ¿Cuál es el ciclo de interacción entre el agente y el entorno?\n",
        "- ¿Qué es un agente? ¿Qué es un entorno?\n",
        "- Conceptos descriptivos sobre qué es:\n",
        "   - Un estado y el espacio de estados.\n",
        "   - Una acción y el espacio de acciones.\n",
        "   - Una recompensa.\n",
        "   - Una política. Distinga entre políticas deterministas y no deterministas.\n",
        "   - Una experiencia.\n",
        "   - Un episodio y una trayectoria. Diferencias entre ambos.\n",
        "- ¿Cuál es la estructura subyacente que se supone que tiene un entorno? Ponga un ejemplo de Proceso de Decisión de Markov sencillo con pocos estados y acciones.\n",
        "- ¿Cuál es el objetivo de un agente en el aprendizaje por refuerzo?\n",
        "   - Concepto de retorno (Return).\n",
        "   - Concepto de factor de descuento $\\gamma$, para qué se usa y su influencia en el valor de retorno con valores altos o bajos.\n",
        "- ¿Qué es el valor de un estado $V(s)$?\n",
        "- ¿Qué es el valor de un par estado-acción $Q(s,a)$?\n",
        "\n",
        "\n",
        "Para responder a estas preguntas puede utilizar cualquier fuente bibliográfica, sea un libro, documento o web. Recuerde **citar las fuentes apropiadamente**.\n",
        "\n",
        "\n",
        "En términos generales: <font color=\"red\">se persigue que el estudiante elabore unos <i>mini-apuntes</i> que le permitan estudiar y aprender qué es el aprendizaje por refuerzo, a nivel introductorio.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El aprendizaje por refuerzo es una de las tres principales categorías en las que se divide el aprendizaje automático. Estas son el aprendizaje supervisado, donde cada entrada tiene una salida predefinida la cual se intenta predecir correctamente, el aprendizaje no supervisado, donde los datos no están etiquetados pero se intentan encontrar patrones en estos, y el aprendizaje por refuerzo, donde los datos no estás etiquetados, pero sí que hay una recompensa o retroalimentación en función de las acciones del agente en el entorno.\n",
        "\n",
        "Los dos elementos principales del aprendizaje por refuerzo son:\n",
        "* Agente: Es el algoritmo de Machine Learning, es quien toma decisiones y acciones en el entorno intentando maximizar la recompensa.\n",
        "* Entorno: Es el espacio de problemas adaptativo con atributos como variables, valores límite, reglas y acciones válidas. Es todo lo que rodea al agente, cambiando su estado y proporcionando recompensas o penalizaciones en función de las decisiones de este. Puede ser estático, o dinámico y cambiar con el tiempo, por lo que las consecuencias de las acciones pueden cambiar. Puede ser un entorno discreto, en un mundo cuadriculado, o continuo, con una cantidad mucho mayor de acciones.\n",
        "\n",
        "El ciclo de interacción entre el agente y el entorno se compone de:\n",
        "* Observación del estado y recopilación de información de este.\n",
        "* Elegir una acción que realizar en función del estado de la política.\n",
        "* Ejecutar la acción.\n",
        "* Transicción de estado. El estado cambia como respuesta a la acción que se ha ejecutado.\n",
        "* Evaluación y recepción de la recompensa o castigo en función del nuevo estado al que se ha llegado.\n",
        "* Actualización del la política, ajustando los parámetros para mejorar la predicción futura.\n",
        "\n",
        "Un estado es una representación de la situación actual del entorno en el que se encuentra el agente, incluyendo información relevante que el agente necesita para tomar decisiones. El espacio de estados es el conjunto completo de todos los posibles estados que el agente puede experimentar en el entorno, puede ser finito cuando el entorno es discreto, como en el ajedrez, o infinito, cuando el entorno es continuo, como en videojuegos con movimiento libre.\n",
        "\n",
        "Una acción es una decisión o movimiento que el agente puede realizar en un estado determinado del entorno, y que le lleva a otro estado. El espacio de acciones es el conjunto completo de todas las acciones posibles que el agente puede tomar en un estado dado.\n",
        "\n",
        "La recompensa es el valor positivo, negativo o cero (es decir, recompensa o castigo) por llevar a cabo una acción. La recompensa acumulada es la suma de todas las recompensas o el valor final. El agente trata de buscar una recompensa óptima, ya que está le acerca a cumplir su meta\n",
        "\n",
        "La política es una función que determina la manera en la que el agente selecciona la acciones. Puede ser determinista, si dado cierto estado la acción que se toma es siempre la misma, o estocástica, si dado un estado hay varias acciones que se pueden tomar, cada una de ellas con cierta probabilidad. Esta última cuenta con la ventaja de que propicia la exploración del entorno.\n",
        "\n",
        "La experiencia se refiere a la información que el agente obtiene a través de su interacción con el entorno. Se compone de los estados, acciones y recompensas obtenidas tras estas, y permite mejorar la toma de nuevas decisiones.\n",
        "\n",
        "Un episodio se refiere a la secuencia completa de interacciones entre un agente y su entorno desde un estado inicial y hasta un estado final. Una trayectoria es la secuencia de estados, acciones y recompensas que el agente experimenta a lo largo de un episodio, representada como una lista de tuplas. A diferencia de un episodio, se centra en los detalles e interacciones específicas que ocurren dentro de este.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Bibliografía:\n",
        "* https://aws.amazon.com/es/what-is/reinforcement-learning/\n",
        "* https://www.datacamp.com/es/tutorial/reinforcement-learning-python-introduction\n",
        "* https://www.freecodecamp.org/espanol/news/introduccion-a-q-learning-aprendizaje-por-refuerzo/\n",
        "* https://openaccess.uoc.edu/bitstream/10609/151792/1/IntroduccionAlAprendizajePorRefuerzo.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AMGSUW_n86y"
      },
      "source": [
        "# **Aprendizaje por refuerzo clásico (4 puntos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07lnZt7gL6L"
      },
      "source": [
        "## La biblioteca *Gymnasium*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WansH0eGhmiC"
      },
      "source": [
        "\n",
        "Su web principal es <a href=\"https://gymnasium.farama.org/index.html\">https://gymnasium.farama.org/index.html</a>\n",
        "\n",
        "- Busque información sobre la biblioteca Gymnasium e instálela.\n",
        "- Instancie la biblioteca con el entorno ```'CliffWalking-v0'```. Describa, para el entorno generado...:\n",
        "  - Cuál es el espacio de estados.\n",
        "  . Cuál es el espacio de acciones.\n",
        "  - Cuál es el sistema de recompensas.\n",
        "  - Cuál es el sistema de terminación/parada de un episodio.\n",
        "  - Para qué sirve el método **reset()**, qué entradas tiene y qué salidas proporciona y qué información contienen las salidas.\n",
        "  - Para qué sirve el método **step()**, qué entradas tiene y qué salidas proporciona, describiendo el significado de cada una de ellas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YfBvCIkho9x"
      },
      "source": [
        "Implemente de forma manual dos políticas, haciendo uso del **wrapper** *TimeLimit* para limitar el número de experiencias en un episodio a 500:\n",
        "\n",
        "- Una función que implemente una política aleatoria.\n",
        "- Una función que implemente una política óptima.\n",
        "\n",
        "Además:\n",
        "\n",
        "- Ejecute ambas políticas un total de 100 episodios y compare, analice y discuta los resultados obtenidos.\n",
        "- Utilice el parámetro **render** para obtener una visualización de un único episodio con cada política. En caso de obtener algún error de ejecución: <a href=\"https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\">Stack Overflow</a> y similares.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rOGrdFimox"
      },
      "source": [
        "## Algoritmos clásicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPoekH2zinxz"
      },
      "source": [
        "- Busque información de algoritmos clásicos de Aprendizaje por Refuerzo (Q-Learning, SARSA, Value Iteration, Policy Iteration, ...).\n",
        "- Seleccione uno de ellos.\n",
        "- Describa su funcionamiento a nivel general, pero con rigor (pasos del algoritmo, función de actualización y qué significa cada término, etc.).\n",
        "\n",
        "En términos generales: <font color=\"red\">se debe describir qué métodos existen desde antes de la incursión de Deep Learning para aprendizaje por refuerzo dando una visión general del área, seleccionar uno de los métodos de aprendizaje y explicarlo en detalle.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhJ0Ha5WjhVC"
      },
      "source": [
        "### Implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ckZv-LZjixa"
      },
      "source": [
        "\n",
        "- Implemente desde cero el algoritmo seleccionado.\n",
        "- Describa su funcionamiento (pasos del algoritmo, función de actualización y qué significa cada término, etc.) **y su relación con la descripción general dada en el apartado anterior**.\n",
        "- Aplíquelo sobre algún entorno de la biblioteca Gymnasium, justificando su idoneidad en términos de espacios de estados y acciones, diseño de recompensas, etc.. Justifique también el uso de hiperparámetros (de haberlos). No olvide también describir el entorno seleccionado (espacio de estados, espacio de acciones, recompensas, etc.)\n",
        "- Discuta los resultados obtenidos considerando el número de veces que el entorno ha sido resuelto tras 100 ejecuciones distintas de prueba de la política obtenida con el algoritmo. Si procede, también puede comparar su implementación con la tabla de clasificación dada en <a href=\"https://github.com/openai/gym/wiki/Leaderboard\">https://github.com/openai/gym/wiki/Leaderboard</a>.\n",
        "\n",
        "En términos generales: <font color=\"red\">se debe usar el algoritmo implementado para resolver un problema, y describir los pasos seguidos con detalle enlazando la implementación con la parte teórica.</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Biblioteca Gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La biblioteca Gymnasium proporciona un entorno para crear y probar agentes de aprendizaje por refuerzo. Ofrece una gran variedad de entornos, desde juegos de texto hasta simulaciones en 3D. Es compatible con muchos algoritmos de aprendizaje por refuerzo y sigue una interfaz estándar.\n",
        "\n",
        "Podemos ver información sobre el entorno Cliff Walking en el siguiente enlace:\n",
        "\n",
        "https://www.gymlibrary.dev/environments/toy_text/cliff_walking/\n",
        "\n",
        "![](https://www.gymlibrary.dev/_images/cliff_walking.gif)\n",
        "\n",
        "Si el agente pisa el precipicio, vuelve al principio. Termina cuando este alcanza la meta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1ZsdOtQIjgHl"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# Instanciar el entorno CliffWalking-v0\n",
        "env = gym.make('CliffWalking-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El tablero es una matriz de 4x12, [3, 0] como la casilla de inicio (parte inferior izquierda), [3, 11] como la meta (parte inferior derecha) y [3, 1..10] como el precipicio (parte inferior central). Si el agente pisa el precipicio, vuelve al principio.\n",
        "\n",
        "Aún que haya 48 casillas, hay 3*12+1 = 37 estados posibles, ya que el agente no puede estar en la colina, ni en la meta ya que implica que finaliza el juego."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Espacio de Estados:\n",
            "Discrete(48)\n"
          ]
        }
      ],
      "source": [
        "# Descripción del espacio de estados\n",
        "print(\"Espacio de Estados:\")\n",
        "print(env.observation_space)  # Espacio de estados (48 estados en total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hay 4 acciones deterministas discretas:\n",
        "* 0: Mover hacia arriba.\n",
        "* 1: Mover a la derecha.\n",
        "* 2: Mover hacia abajo.\n",
        "* 3: Mover a la izquierda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Espacio de Acciones:\n",
            "Discrete(4)\n"
          ]
        }
      ],
      "source": [
        "# Descripción del espacio de acciones\n",
        "print(\"\\nEspacio de Acciones:\")\n",
        "print(env.action_space)  # Espacio de acciones (4 acciones posibles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostramos un ejemplo de acción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estado Inicial: (36, {'prob': 1})\n",
            "\n",
            "Resultados del paso:\n",
            "Siguiente Estado: 24\n",
            "Recompensa: -1\n",
            "Episodio terminado: False\n",
            "Información adicional: {'prob': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Reiniciar el entorno\n",
        "initial_state = env.reset()\n",
        "print(\"\\nEstado Inicial:\", initial_state)\n",
        "\n",
        "# Ejemplo de uso del método step()\n",
        "action = 0  # Acción: mover hacia arriba\n",
        "next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "print(\"\\nResultados del paso:\")\n",
        "print(\"Siguiente Estado:\", next_state)\n",
        "print(\"Recompensa:\", reward)\n",
        "print(\"Episodio terminado:\", terminated)\n",
        "print(\"Información adicional:\", info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cada paso en el tiempo conlleva -1 de recompensa, y pisar el precipicio implica -100 de recompensa. El episodio termina cuando el agente ha llegado a la casilla de meta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Método reset()**\n",
        "\n",
        "En la documentación podemos ver información sobre este método:\n",
        "\n",
        "https://gymnasium.farama.org/api/env/#gymnasium.Env.reset\n",
        "\n",
        "Se utiliza para reiniciar el entorno a su estado inicial. Acepta los siguientes parámetros:\n",
        "* seed (optional int): Se utiliza para establecer una semilla para la generación de números aleatorios. Puede ser útil para reproducir los mismos resultados en experimentos.\n",
        "* options (optional dict): Información adicional para especificar cómo se restablece el entorno.\n",
        "\n",
        "Devuelve las siguientes salidas:\n",
        "* observation (ObsType): Es el estado inicial del entorno después de la reinicialización.\n",
        "* info (dictionary): Un diccionario que puede contener información adicional sobre el estado del entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method reset in module gymnasium.wrappers.common:\n",
            "\n",
            "reset(*, seed: 'int | None' = None, options: 'dict[str, Any] | None' = None) -> 'tuple[ObsType, dict[str, Any]]' method of gymnasium.wrappers.common.OrderEnforcing instance\n",
            "    Resets the environment with `kwargs`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(env.reset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Método step()**\n",
        "\n",
        "En la documentación podemos ver información de este método:\n",
        "\n",
        "https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
        "\n",
        "Ejecuta un paso temporal en el entorno utilizando las acciones del agente. Sus parámetros de entrada son:\n",
        "* action (ActType): La acción que realizará el agente.\n",
        "\n",
        "Devuelve las siguientes salidas:\n",
        "* observation (ObsType): Cómo será el entorno después de que se haya ejecutado la acción.\n",
        "* reward (SupportsFloat): La recompensa obtenida como resultado de la acción tomada.\n",
        "* terminated (bool): Un indicador que señala si el episodio ha terminado.\n",
        "* truncated (bool): Si se pasa el límite de tiempo o el agente sale de los límites.\n",
        "* info (dict): Contiene información auxiliar de diagnóstico, por ejemplo métricas que describen el estado de rendimiento del agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method step in module gymnasium.wrappers.common:\n",
            "\n",
            "step(action: 'ActType') -> 'tuple[ObsType, SupportsFloat, bool, bool, dict]' method of gymnasium.wrappers.common.OrderEnforcing instance\n",
            "    Steps through the environment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(env.step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Implementación de políticas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos las dos funciones con las políticas, la random, que ejecuta movimientos aleatorios, y la política óptima, que se mueve a la posición que más cerca le deja de la meta en cada movimiento, sin llevar a cabo movimientos que provoquen el reinicio del agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_policy(state, env):\n",
        "    action = env.action_space.sample()  # Selecciona una acción aleatoria\n",
        "    return action\n",
        "\n",
        "def optimal_policy(state, env):\n",
        "    # Convertir el estado a coordenadas (fila, columna)\n",
        "    row = state // 12\n",
        "    col = state % 12\n",
        "\n",
        "    # Coordenadas de la meta (3, 11)\n",
        "    goal_row = 3\n",
        "    goal_col = 11\n",
        "\n",
        "    # Función para calcular la distancia a la meta\n",
        "    def distance_to_goal(r, c):\n",
        "        return abs(goal_row - r) + abs(goal_col - c)\n",
        "\n",
        "    # Inicializar variables para la mejor acción y la mejor distancia\n",
        "    best_action = None\n",
        "    best_distance = float('inf')\n",
        "\n",
        "    # Posibles movimientos: (acción, nueva fila, nueva columna)\n",
        "    possible_moves = [\n",
        "        (0, row - 1, col),  # Arriba\n",
        "        (1, row, col + 1),  # Derecha\n",
        "        (2, row + 1, col),  # Abajo\n",
        "        (3, row, col - 1)   # Izquierda\n",
        "    ]\n",
        "\n",
        "    for action, new_row, new_col in possible_moves:\n",
        "        # Verificar si el movimiento está dentro de los límites del mapa\n",
        "        if new_row < 0 or new_row > 3 or new_col < 0 or new_col > 11:\n",
        "            continue  # Ignorar este movimiento, está fuera del mapa\n",
        "\n",
        "        # Verificar si el movimiento es seguro (no caer en el cliff)\n",
        "        if new_row == 3 and 1 <= new_col <= 10:\n",
        "            continue  # Ignorar este movimiento, es peligroso\n",
        "\n",
        "        # Calcular la distancia a la meta\n",
        "        dist = distance_to_goal(new_row, new_col)\n",
        "\n",
        "        # Si la distancia es mejor que la mejor encontrada, actualizar\n",
        "        if dist < best_distance:\n",
        "            best_distance = dist\n",
        "            best_action = action\n",
        "\n",
        "    return best_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos la función que ejecuta una política en el entorno y calcula la suma de recompensas de cada acción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_policy(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    terminated = False\n",
        "\n",
        "    while not terminated:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = policy(state, env)\n",
        "        state, reward, terminated, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuramos el entorno y ejecutamos las dos políticas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recompensa política aleatoria: -66378.18\n",
            "Recompensa política óptima: -13.0\n"
          ]
        }
      ],
      "source": [
        "# Configuración del entorno con el wrapper TimeLimit a 500\n",
        "env = gym.make('CliffWalking-v0', max_episode_steps=500)\n",
        "\n",
        "# Ejecutar ambas políticas durante 100 episodios\n",
        "num_episodes = 100\n",
        "random_rewards = []\n",
        "optimal_rewards = []\n",
        "\n",
        "# Ejecutar política aleatoria\n",
        "for _ in range(num_episodes):\n",
        "    reward = run_policy(env, random_policy)\n",
        "    random_rewards.append(reward)\n",
        "\n",
        "# Ejecutar política óptima\n",
        "for _ in range(num_episodes):\n",
        "    reward = run_policy(env, optimal_policy)\n",
        "    optimal_rewards.append(reward)\n",
        "\n",
        "# Cerrar el entorno\n",
        "env.close()\n",
        "\n",
        "# Análisis de resultados\n",
        "print(f\"Recompensa política aleatoria: {np.mean(random_rewards)}\")\n",
        "print(f\"Recompensa política óptima: {np.mean(optimal_rewards)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que la suma de recompensas de la política óptima es -13, que corresponde a 13 pasos que da el agente entre el inicio y la meta, mientras que en la política aleatoria es mucho mayor, ya que da muchos pasos hasta llegar a la meta, muchos de ellos incluso fuera del mapa o en el precipicio, restando aún más puntos de recompensa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejecutamos de forma visual un episodio de la política random:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('CliffWalking-v0', max_episode_steps=500, render_mode=\"human\")\n",
        "\n",
        "# Visualizar episodio con política aleatoria (Tarda demasiado en ejecutarse así que no lo mostraremos)\n",
        "#print(\"Episodio con política aleatoria:\")\n",
        "#run_policy(env, random_policy, render=True)\n",
        "#env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejecutamos de forma visual un episodio de la política óptima y vemos como en esta el agente sigue el camino más corto hasta la meta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episodio con política óptima:\n",
            "-13\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CliffWalking-v0', max_episode_steps=500, render_mode=\"human\")\n",
        "\n",
        "# Visualizar episodio con política óptima\n",
        "print(\"Episodio con política óptima:\")\n",
        "print(run_policy(env, optimal_policy, render=True))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algoritmo clásico: Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los algoritmos clásicos de aprendizaje por refuerzo, que han sido fundamentales antes de la popularización del aprendizaje profundo, permiten a un agente aprender a tomar decisiones óptimas en un entorno. Funcionan mediante la estimación de la calidad de las acciones.\n",
        "\n",
        "La principal diferencia entre los métodos clásicos de aprendizaje por refuerzo y los de aprendizaje profundo radica en cómo representan y procesan la información, y en su capacidad para manejar entornos complejos. Los métodos clásicos utilizan tablas o funciones simples para representar el conocimiento, mientras los de aprendizaje profundo emplean redes neuronales profundas para representar funciones de valor o políticas.\n",
        "\n",
        "Algunos de estos son Q-Learning, SARSA, Value Iteration, Policy Iteration, métodos de Monte Carlo...\n",
        "\n",
        "En este caso vamos a estudiar el algoritmo **Q-Learning**:\n",
        "\n",
        "Este algoritmo de aprendizaje por refuerzo usa el método de ensayo y error para determinar qué acciones producen recompensas o penalizaciones. Utiliza una Q-table, que almacena Q-values que representan las recompensas esperadas por realizar determinadas acciones en estados concretos.\n",
        "\n",
        "El agente pasa por diferentes estados realizando acciones, recibiendo recompensas y actualizando la Q-table. El episodio continúa hasta que el agente alcanza un estado terminal.\n",
        "\n",
        "Los Q-values se actualizan usando la siguiente fórmula:\n",
        "\n",
        "$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) $\n",
        "\n",
        "* S es el estado actual.\n",
        "* A es la acción que ha tomado el agente.\n",
        "* S' es el próximo estado al que se mueve el agente.\n",
        "* A' es una acción asociada al estado S'.\n",
        "* R es la recompensa de ejecutar la acción A en el estado S.\n",
        "* γ (Gamma) el factor de descuento. Toma un valor entre 0 y 1. Un valor cercano a 0 significa que prioriza las recompensas inmediatas sobre las futuras, mientras un valor cercano a 1 indica que el agente considera las recompensas futuras casi tan importantes como las inmediatas.\n",
        "* α (Alpha) es la tasa de aprendizaje. Un valor cercano a 0 significa que el agente aprenderá muy lentamente, mientras un valor cercano a 1 le hará aprender rápidamente.\n",
        "\n",
        "La Q-table es una tabla en la que el agente almacena información sobre qué acciones producen las mejores recompensas en cada estado. A medida que el agente explora y aprende de sus interacciones con el entorno, actualiza sus valores. Las filas representan los estados y las columnas las acciones posibles. Cada entrada de la tabla corresponde al Q-value de un par estado acción.\n",
        "\n",
        "Política ϵ-greedy: Esta política permite tomar al agente decisiones más o menos aleatorias:\n",
        "* Explotación: Con una probabilidad de 1-ϵ, el agente elegirá la acción con el Q-value más alto, es decir, usará su conocimiento actual para maximizar la recompensa.\n",
        "* Exploración: Con probabilidad ϵ, el agente elige una acción al azar, explorando nuevas posibilidades para aprender si hay mejores formas de obtener recompensas.\n",
        "\n",
        "El algoritmo sigue los siguiente pasos:\n",
        "* Inicialización: El agente comienza con una tabla Q inicial, donde los valores se inicializan a cero.\n",
        "* Exploración: Elige una acción basada en la política ϵ-greedy, explorar o explotar.\n",
        "* Acción y actualización: Realiza la acción, observa el siguiente estado y recibe una recompensa. El Q-value correpondiente se actualiza.\n",
        "* Iteración: El proceso se repite durante varios episodios hasta que el agente aprende la política óptima.\n",
        "\n",
        "Bibliografía:\n",
        "* https://www.geeksforgeeks.org/q-learning-in-python/\n",
        "\n",
        "La elección de los parámetros es la siguiente:\n",
        "* Tasa de aprendizaje alpha 0.1: No es un valor muy alto ni demasiado bajo, de forma que el agente aprenderá de forma gradual y convergerá a un ritmo adecuado a una política óptima.\n",
        "* Factor de descuento gamma 0.6: Es un valor intermedio entre valorar las recompensas futuras y las inmediatas, un poco más cerca de 1 por lo que dará un poco más de valor a las futuras.\n",
        "* Tasa de exploración epsilon 0.1: Un valor de 0.1 significa que el 10% de las acciones tomadas serán aleatorias, de forma parte de las decisiones serán más diferentes, aumentando así la exploración. Este equilibrio es importante para que el agente no se quede atrapado en una política subóptima.\n",
        "\n",
        "A continuación vemos la implementación de este algoritmo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "# Tabla Q y función para resetear los valores de esta\n",
        "Q = None\n",
        "def reset_Q_table(env):\n",
        "    global Q\n",
        "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Algoritmo Q-Learning\n",
        "def qlearning(env, num_episodes = 10000, verbose = False, visual = False):\n",
        "    # Parámetros\n",
        "    alpha = 0.1  # Tasa de aprendizaje\n",
        "    gamma = 0.6  # Factor de descuento\n",
        "    epsilon = 0.1  # Tasa de exploración\n",
        "\n",
        "    total_reward = 0\n",
        "    n_goal_achieved = 0\n",
        "\n",
        "    # Ejecución de cada episodio\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        terminated = False\n",
        "        episode_reward = 0\n",
        "        \n",
        "        while not terminated:\n",
        "            # Muéstra gŕaficamente la ejecución si el parámetro visual es True\n",
        "            if visual:\n",
        "                env.render()\n",
        "\n",
        "            # Elije una acción\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()  # Exploración\n",
        "            else:\n",
        "                action = np.argmax(Q[state])  # Explotación\n",
        "            \n",
        "            # Ejecuta la acción\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            # Actualizar la tabla Q\n",
        "            Q[state][action] = Q[state][action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            total_reward += reward\n",
        "\n",
        "        # Comprobamos si ha llegado a la meta\n",
        "        goal_achieved = terminated and not truncated\n",
        "        if goal_achieved:\n",
        "            n_goal_achieved += 1\n",
        "\n",
        "        # Mostramos datos de cada episodio\n",
        "        if verbose:\n",
        "            print(f\"{episode_reward} ({goal_achieved}), \", end=\"\")\n",
        "\n",
        "    # Mostramos datos generales de la ejecución\n",
        "    print()\n",
        "    print(f\"Recompensa media tras {num_episodes} episodios: {total_reward / num_episodes}\")\n",
        "    print(f\"Nº de veces que ha llegado a la meta: {n_goal_achieved}/{num_episodes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para probar el algoritmo usaremos el entorno Taxi-v3. Se puede ver su documentación en el siguiente enlace:\n",
        "\n",
        "https://www.gymlibrary.dev/environments/toy_text/taxi/#taxi\n",
        "\n",
        "![](https://www.gymlibrary.dev/_images/taxi.gif)\n",
        "\n",
        "Hay cuatro lugares designados en el mundo cuadriculado. Al empezar el episodio, el taxi y el pasajero están en diferentes ubicaciones aleatorias. El taxi debe conducir hasta la ubicación del pasajero, recogerlo, y conducir hasta el destino del pasajero y dejarlo. Una vez deja al pasajero, el episodio termina.\n",
        "\n",
        "Hay 6 acciones deterministas discretas:\n",
        "* 0: Moverse al sur.\n",
        "* 1: Moverse al norte.\n",
        "* 2: Moverse al este.\n",
        "* 3: Moverse al oeste.\n",
        "* 4: Recoge al pasajero.\n",
        "* 5: Deja al pasajero.\n",
        "\n",
        "Hay 500 estados discretos, ya que hay 25 posiciones de taxi, 5 posibles ubicaciones del pasajero (incluido el caso en que el pasajero está en el taxi) y 4 ubicaciones de destino. Cada espacio de estados está representado por la tupla (fila taxi, columna taxi, ubicación pasajero, destino).\n",
        "\n",
        "Las recompensas son:\n",
        "* -1 por cada paso a menos que se active otra recompensa.\n",
        "* +20 por entregar el pasajero.\n",
        "* -10 por ejecutar acciones de recoger y dejar ilegalmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejecutamos el algoritmo para este entorno, con un máximo de 300 pasos por episodio, y 100 episodios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1410 (False), -1424 (False), -1047 (False), -2451 (False), -1133 (False), -415 (False), -2071 (False), -1223 (False), -239 (True), -2199 (False), -491 (False), -1046 (False), -822 (False), -750 (False), -1177 (False), -727 (False), -114 (True), -1406 (False), -649 (False), -1177 (False), -574 (False), -630 (False), -406 (True), -837 (False), -194 (True), -1254 (False), -1135 (False), -601 (False), -217 (True), -400 (True), -1029 (False), -550 (False), -30 (True), -379 (True), -254 (True), -2397 (False), -1949 (False), -860 (False), -762 (False), -905 (False), -835 (False), -164 (True), -1194 (False), -505 (False), -736 (False), -275 (True), -469 (False), -1108 (False), -1221 (False), -1102 (False), -30 (True), -600 (False), -845 (False), -45 (True), -48 (True), -377 (True), -413 (False), -758 (False), -336 (True), -368 (False), -530 (False), -271 (True), -165 (True), -657 (False), -838 (False), -175 (True), -706 (False), -539 (False), -286 (True), -351 (True), -120 (True), -749 (False), -233 (True), -502 (False), -215 (True), -268 (True), -435 (False), -221 (True), -373 (True), -338 (True), -515 (False), -666 (False), -315 (True), -449 (False), -461 (False), -586 (False), -867 (False), -120 (True), -477 (False), -394 (True), -200 (True), -356 (True), -72 (True), -284 (True), -240 (True), -280 (True), -446 (False), -277 (True), -208 (True), -254 (True), \n",
            "Recompensa media tras 100 episodios: -642.02\n",
            "Nº de veces que ha llegado a la meta: 40/100\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v3\", max_episode_steps=300)\n",
        "reset_Q_table(env)\n",
        "qlearning(env, num_episodes=100, verbose=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos como en los primeros episodios no suele alcanzar la meta, mientras en los finales hay más probabilidades de que la alcance. En total, llega un número cercano a 30 veces a la meta (cambia en cada ejecución ya que los resultados son aleatorios).\n",
        "\n",
        "La recompensa media en estos primeros 100 episodios es muy negativa, ya que al tratarse de las primeras ejecuciones aún no se ha acercado a la solución óptima de la Q-table, por lo que el agente debe dar muchos pasos hasta llegar a la meta en cada episodio.\n",
        "\n",
        "A continuación vamos a entrenar de nuevo la Q-table, pero esta vez con 20000 episodios para obtener mejores resultados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Recompensa media tras 20000 episodios: -6.0327\n",
            "Nº de veces que ha llegado a la meta: 19834/20000\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v3\",)\n",
        "reset_Q_table(env)\n",
        "qlearning(env, num_episodes=20000)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La recompensa media que hemos obtenido ahora es mucho más alta, siendo un valor que se acerca a 0. Probamos los resultados de 100 nuevos episodios una vez contamos con el entrenamiento anterior realizado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Recompensa media tras 100 episodios: 2.07\n",
            "Nº de veces que ha llegado a la meta: 100/100\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v3\")\n",
        "qlearning(env, num_episodes=100)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El valor obtenido es mucho mejor, esta vez por encima de 0, sin embargo aún no alcanza los mismos valores que los algoritmos ejecutados en condiciones similares en este leaderboard para taxi-v3:\n",
        "\n",
        "https://github.com/openai/gym/wiki/Leaderboard#taxi-v3\n",
        "\n",
        "Ahora vamos a ver una ejecución de forma visual, usando de nuevo el Q-table que habíamos entrenado previamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Recompensa media tras 1 episodios: 11.0\n",
            "Nº de veces que ha llegado a la meta: 1/1\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
        "qlearning(env, num_episodes=1, visual=True)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por último, para demostrar que funciona con otros entornos, probaremos el algoritmo con FrozenLake-v1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Recompensa media tras 20000 episodios: 0.02525\n",
            "Nº de veces que ha llegado a la meta: 19993/20000\n",
            "\n",
            "Recompensa media tras 1 episodios: 1.0\n",
            "Nº de veces que ha llegado a la meta: 1/1\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"FrozenLake-v1\")\n",
        "reset_Q_table(env)\n",
        "qlearning(env, num_episodes=20000)\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
        "qlearning(env, num_episodes=1, visual=True)\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
